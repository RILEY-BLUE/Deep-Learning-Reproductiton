{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Blog.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "deepnote_notebook_id": "fdf379ac-8f91-4d86-b34e-67e5f64cfa3a",
    "deepnote": {},
    "deepnote_execution_queue": []
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00000-b9b5f5bb-82fc-4f74-b1f9-986b70ac6216",
        "deepnote_cell_type": "markdown",
        "id": "ycXKtdORr9YS"
      },
      "source": [
        "<h1>Reproduction Blog: Deep Fruit Detection in Orchards</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZYQn-9ifwdc"
      },
      "source": [
        "Author(s):  \n",
        "Yulei Qiu (5233178) y.qiu-7@student.tudelft.nl   \n",
        "Yurui Du (5217849) y.du-7@student.tudelft.nl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00001-31297292-1d08-4d47-8098-341a9b3600fe",
        "deepnote_cell_type": "markdown",
        "id": "dczitxy-r9YT"
      },
      "source": [
        "<p>The goal of this blog post is to present our attempt to reproduce the paper <a href=\"https://arxiv.org/abs/1610.03677\">Deep Fruit Detection in Orchards</a>, written by Suchet Bargoti and James Underwood, 2016. This paper describes the use of a state-of-the-art object detection framework, Faster R-CNN, in the context of fruit detection in orchards, including mangoes, almonds and apples. Throughout this notebook, we explain our efforts to reproduce Fig. 2 of the paper. We referred to some existing implementation of Faster R-CNN. As we could certainly not use and run them directly, we implemented some new code variants throughout this project.</p>\n",
        "\n",
        "<p><em>We are one of the reproduction groups of the course CS4240 Deep Learning of Delft University of Technology. More reproduce works of our fellow students can be found <a href=\"https://reproducedpapers.org/\">here</a></em>.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00002-1b65f462-ba21-4bbf-af71-f27ff24efbdf",
        "deepnote_cell_type": "markdown",
        "id": "fTLWDXmEr9YV"
      },
      "source": [
        "<h2>Architecture</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00003-80aea699-a5fc-485d-acdb-4cb890a1bc8e",
        "deepnote_cell_type": "markdown",
        "id": "bmfcijrPr9YV"
      },
      "source": [
        "<p>According to the paper <a href=\"https://arxiv.org/abs/1506.01497\">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a>, the architecture of the Faster R-CNN consists of the following parts:</p>\n",
        "<ol>\n",
        "<li>Input: images, represented by Height × Width × Depth tensors</li>\n",
        "<li>Convolutional layer: a pre-trained CNN, ending up with a convolutional feature map</li>\n",
        "<li>Region Proposal Networks (RPN): takes the feature as input and outputs a set of rectangular object proposals (bounding boxes), which may contain objects</li>\n",
        "<li>Region of Interest (RoI) Pooling: extracts those features which would correspond to the relevant objects into a new tensor\n",
        "<li>R-CNN module: classifies the content in the bounding box and adjusts the bounding box coordinates\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00005-61f4a700-2c96-4c5a-ad4c-6cd8d8d9a958",
        "deepnote_cell_type": "markdown",
        "id": "YmnY8u2xr9YW"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1wTRGqd3FMwKm8nTjpLhq04CUXACCkYWh)\n",
        "Source: https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00004-a06ac20c-3e4e-4d0a-a61d-25dd44ea1c75",
        "deepnote_cell_type": "markdown",
        "id": "stSWDCicr9YW"
      },
      "source": [
        "<h2>Data</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00005-71d26216-9f7b-40e1-bf8f-0f6fb600f69d",
        "deepnote_cell_type": "markdown",
        "id": "kEChnx-lr9YX"
      },
      "source": [
        "<p>The dataset is provide by the authors of the paper, which is accessible from <a href=\"http://data.acfr.usyd.edu.au/ag/treecrops/2016-multifruit/\">here</a>. You can download the dataset directly. The labeller is just for visualization.</p>\n",
        "\n",
        "<p>We choose to work on the almond dataset, which contains 620 images (300 x 300 pixels). The annotations are already done and saved as .csv files in another folder. More specificly, they are bounding boxes including every fruit object in each images. To further extend the dataset, we perform data augmentation, which will be explained in datail later.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00008-c7d6c496-e1f3-4d1a-990d-448e25f454e7",
        "deepnote_cell_type": "markdown",
        "id": "Xj8uCQiHr9YY"
      },
      "source": [
        "<h2>Approach</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00009-b83174ef-d2ec-4f5b-b86e-31f0b9d7459a",
        "deepnote_cell_type": "markdown",
        "id": "BsIjbOewr9YY"
      },
      "source": [
        "<p>There is no existing code for the paper <a href=\"https://arxiv.org/abs/1610.03677\">Deep Fruit Detection in Orchards</a>. But, there is some existing implementation of Faster R-CNN. Therefore, we decide not to do the reproduction from scratch. We refer to the code of Faster R-CNN and implement the network on our own.</p>\n",
        "<p>We then train the Faster R-CNN and plot the accuracy and loss. The test is done on a smaller dataset. We show the intermediate images in test process and reproduce Fig. 2</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00008-becff1e6-6a13-45ac-b451-6bdeae5588e8",
        "deepnote_cell_type": "markdown",
        "id": "lb66FfpTr9YY"
      },
      "source": [
        "<h2>Reproducing Fig. 2</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00007-570789ab-fd23-4b7d-91c0-fb29e183ad76",
        "deepnote_cell_type": "markdown",
        "id": "1kszDsEYr9YZ"
      },
      "source": [
        "<p>The figure below shows the Faster R-CNN network in the paper <a href=\"https://arxiv.org/abs/1610.03677\">Deep Fruit Detection in Orchards</a>. We will follow the flow chart and re-implement the network.</p>\n",
        "\n",
        "![](https://drive.google.com/uc?id=1hIinMOaoM39GJvkk6j9wenYEmGJ7CwGw)\n",
        "<p>Fig. 2. The Faster R-CNN Network. A 3-channel input image is propagated through a set of convolutional layers, from which Region of Interest boxes are proposed (dashed red boxes, with one high object probability box highlighted as an example). Each box is propagated through fully connected layers, which return their class probability and regresses a finer bounding box around individual objects (solid red boxes). Ground truth from the input image (in blue) is used in the RPN and the R-CNN layers during training. During testing, a class specific detection threshold is applied to the output, followed by Non-Maximum Suppression to remove overlapping results.</p>\n",
        "Credit: <a href=\"https://arxiv.org/abs/1506.01497\">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00010-5097671a-1e6a-4ec0-8c39-1d3f1d360f9a",
        "deepnote_cell_type": "markdown",
        "id": "BJhGIr3gr9YZ"
      },
      "source": [
        "<h3>Convolutional Layers</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00014-b9951c1c-32b1-46f3-9406-64969bdd1750",
        "deepnote_cell_type": "markdown",
        "id": "a0L2X7Z4r9Ya"
      },
      "source": [
        "<p>The authors experiment with ZF and VGG16 network as the convolutional layers. Here, we use VGG16, which contains 13 convolutional layers. We define VGG16 as the base network here so that the weights of convolutional layers can be shared between the RPN and the R-CNN components.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKgP6QsqvIho",
        "cell_id": "00007-f747afb4-e86c-45da-8f47-6ca24475f687",
        "deepnote_cell_type": "code"
      },
      "source": [
        "def nn_base(input_tensor=None, trainable=False):\n",
        "\n",
        "\n",
        "    input_shape = (None, None, 3)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    bn_axis = 3\n",
        "\n",
        "    # Block 1\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00013-27178767-7c36-4df4-856e-fed497da3cea",
        "deepnote_cell_type": "markdown",
        "id": "M2IWMklfr9Yb"
      },
      "source": [
        "<h3>Region Proposal Network (RPN)</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00016-65ebfb48-70be-480d-ad39-49f07830db82",
        "deepnote_cell_type": "markdown",
        "id": "XPXu62Wkr9Yc"
      },
      "source": [
        "<p>Local regions in the feature map are forward propagated into two sibling fully connected layers, a box-regression layer and a box-classification layer. This is the RPN layer, and the fixed number of class agnostic detections are the object proposals.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7rjNIUBvLyv",
        "cell_id": "00008-51610662-d93f-4833-b10a-6e04e0104760",
        "deepnote_cell_type": "code"
      },
      "source": [
        "def rpn_layer(base_layers, num_anchors):\n",
        "    \"\"\"Create a rpn layer\n",
        "        Step1: Pass through the feature map from base layer to a 3x3 512 channels convolutional layer\n",
        "                Keep the padding 'same' to preserve the feature map's size\n",
        "        Step2: Pass the step1 to two (1,1) convolutional layer to replace the fully connected layer\n",
        "                classification layer: num_anchors (9 in here) channels for 0, 1 sigmoid activation output\n",
        "                regression layer: num_anchors*4 (36 in here) channels for computing the regression of bboxes with linear activation\n",
        "    Args:\n",
        "        base_layers: vgg in here\n",
        "        num_anchors: 9 in here\n",
        "\n",
        "    Returns:\n",
        "        [x_class, x_regr, base_layers]\n",
        "        x_class: classification for whether it's an object\n",
        "        x_regr: bboxes regression\n",
        "        base_layers: vgg in here\n",
        "    \"\"\"\n",
        "    \n",
        "    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n",
        "\n",
        "    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n",
        "    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n",
        "\n",
        "    return [x_class, x_regr, base_layers]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00016-f87a3c0f-e1fb-4cc1-bd7e-dde26fef1d54",
        "deepnote_cell_type": "markdown",
        "id": "AC4h37Gjr9Yd"
      },
      "source": [
        "<h3>Region of Interest (RoI) Pooling</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00012-4b2184ba-097d-420b-b4d9-170c9a4afb64",
        "deepnote_cell_type": "markdown",
        "id": "wS2AfyfVr9Yd"
      },
      "source": [
        "<p>Next, we implement the ROI Pooling. The purpose of ROI Pooli is to perform max pooling on inputs of nonuniform sizes to obtain fixed-size feature maps</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOFcfmJLvB0D",
        "cell_id": "00006-a11a1f76-ef5d-4fc5-90d8-85aa4da2162a",
        "is_code_hidden": false,
        "deepnote_cell_type": "code"
      },
      "source": [
        "class RoiPoolingConv(Layer):\n",
        "    '''ROI pooling layer for 2D inputs.\n",
        "    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n",
        "    K. He, X. Zhang, S. Ren, J. Sun\n",
        "    # Arguments\n",
        "        pool_size: int\n",
        "            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n",
        "        num_rois: number of regions of interest to be used\n",
        "    # Input shape\n",
        "        list of two 4D tensors [X_img,X_roi] with shape:\n",
        "        X_img:\n",
        "        `(1, rows, cols, channels)`\n",
        "        X_roi:\n",
        "        `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n",
        "    # Output shape\n",
        "        3D tensor with shape:\n",
        "        `(1, num_rois, channels, pool_size, pool_size)`\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, pool_size, num_rois, **kwargs):\n",
        "\n",
        "        self.dim_ordering = K.image_data_format()\n",
        "        self.pool_size = pool_size\n",
        "        self.num_rois = num_rois\n",
        "\n",
        "        super(RoiPoolingConv, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.nb_channels = input_shape[0][3]   \n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "\n",
        "        assert(len(x) == 2)\n",
        "\n",
        "        # x[0] is image with shape (rows, cols, channels)\n",
        "        img = x[0]\n",
        "\n",
        "        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n",
        "        rois = x[1]\n",
        "\n",
        "        input_shape = K.shape(img)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        for roi_idx in range(self.num_rois):\n",
        "\n",
        "            x = rois[0, roi_idx, 0]\n",
        "            y = rois[0, roi_idx, 1]\n",
        "            w = rois[0, roi_idx, 2]\n",
        "            h = rois[0, roi_idx, 3]\n",
        "\n",
        "            x = K.cast(x, 'int32')\n",
        "            y = K.cast(y, 'int32')\n",
        "            w = K.cast(w, 'int32')\n",
        "            h = K.cast(h, 'int32')\n",
        "\n",
        "            # Resized roi of the image to pooling size (7x7)\n",
        "            rs = tf.image.resize(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n",
        "            outputs.append(rs)\n",
        "                \n",
        "\n",
        "        final_output = K.concatenate(outputs, axis=0)\n",
        "\n",
        "        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n",
        "        # Might be (1, 4, 7, 7, 3)\n",
        "        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n",
        "\n",
        "        # permute_dimensions is similar to transpose\n",
        "        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
        "\n",
        "        return final_output\n",
        "    \n",
        "    \n",
        "    def get_config(self):\n",
        "        config = {'pool_size': self.pool_size,\n",
        "                  'num_rois': self.num_rois}\n",
        "        base_config = super(RoiPoolingConv, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00019-788559e7-48f6-4418-bbd5-467e74375d84",
        "deepnote_cell_type": "markdown",
        "id": "I3WqD-3Vr9Ye"
      },
      "source": [
        "<p>The following block utilizes ROI Pooling to extract fixed-sized feature maps for each proposal, together with final step in Faster R-CNN, where we use these features for classification. A fully-connected layer is used to output a score for each possible object class. The authors of Faster R-CNN takes the feature map for each proposal, flattens it and uses two fully-connected layers of size 4096 with ReLU activation. </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-rCW87BvPb5",
        "cell_id": "00009-b224a34a-3f31-451c-9a1b-d7774d8dd673",
        "deepnote_cell_type": "code"
      },
      "source": [
        "def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 4):\n",
        "    \"\"\"Create a classifier layer\n",
        "    \n",
        "    Args:\n",
        "        base_layers: vgg\n",
        "        input_rois: `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n",
        "        num_rois: number of rois to be processed in one time (4 in here)\n",
        "\n",
        "    Returns:\n",
        "        list(out_class, out_regr)\n",
        "        out_class: classifier layer output\n",
        "        out_regr: regression layer output\n",
        "    \"\"\"    \n",
        "\n",
        "    input_shape = (num_rois,7,7,512)\n",
        "\n",
        "    pooling_regions = 7\n",
        "\n",
        "    # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)\n",
        "    # num_rois (4) 7x7 roi pooling\n",
        "    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n",
        "\n",
        "    # Flatten the convlutional layer and connected to 2 FC and 2 dropout\n",
        "    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\n",
        "    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)\n",
        "    out = TimeDistributed(Dropout(0.5))(out)\n",
        "    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)\n",
        "    out = TimeDistributed(Dropout(0.5))(out)\n",
        "\n",
        "    # There are two output layer\n",
        "    # out_class: softmax acivation function for classify the class name of the object\n",
        "    # out_regr: linear activation function for bboxes coordinates regression\n",
        "    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n",
        "    # note: no regression target for bg class\n",
        "    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n",
        "\n",
        "    return [out_class, out_regr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00026-8611b81f-64b7-43bc-b1d8-5b13d38d96a9",
        "deepnote_cell_type": "markdown",
        "id": "6b26HLRFr9Yg"
      },
      "source": [
        "<h3>Loss function</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00036-43d305df-1add-4348-bc46-a7a71d0216b2",
        "deepnote_cell_type": "markdown",
        "id": "pPz8lsjWr9Yg"
      },
      "source": [
        "<p>Since the RPN have two predictions, classification and regression, we should define the loss functions for them. For regression, the authors use Smooth L1 loss function to increase the convergence rate. For classification, they calculate the loss using binary cross entropy.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV4s1H_ex82k"
      },
      "source": [
        "<h3>Non-Maximum-Suppression</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00016-ada1a54e-f3a1-4b3c-949d-9d295c850836",
        "deepnote_cell_type": "markdown",
        "id": "Rt1xNzYgr9Yi"
      },
      "source": [
        "This following block implements the Non-Maximum-Suppression (NMS) to handle overlapping detections. The original code is by <a href=\"https://github.com/rbgirshick/voc-dpm/blob/master/test/nms.m\">Felzenszwalb et al </a>. Here we use the faster version which replaces for loops with vectorized operations for 100 times speed-up. \n",
        "\n",
        "The overlapping threshold is adopting the optimized value for fruit detection suggested by <a href=\"https://arxiv.org/abs/1610.03677\">Deep Fruit Detection in Orchards</a>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cu_uScDvzvA",
        "cell_id": "00016-2431d875-7dfa-46e8-a8ad-a4ec85f35a69",
        "deepnote_cell_type": "code"
      },
      "source": [
        "def non_max_suppression_fast(boxes, probs, overlap_thresh=0.3, max_boxes=300):\n",
        "    # code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
        "    # if there are no boxes, return an empty list\n",
        "\n",
        "     \n",
        "    if len(boxes) == 0:\n",
        "        return []\n",
        "\n",
        "    # grab the coordinates of the bounding boxes\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    np.testing.assert_array_less(x1, x2)\n",
        "    np.testing.assert_array_less(y1, y2)\n",
        "\n",
        "    # if the bounding boxes integers, convert them to floats --\n",
        "    # this is important since we'll be doing a bunch of divisions\n",
        "    if boxes.dtype.kind == \"i\":\n",
        "        boxes = boxes.astype(\"float\")\n",
        "\n",
        "    # initialize the list of picked indexes\t\n",
        "    pick = []\n",
        "\n",
        "    # calculate the areas\n",
        "    area = (x2 - x1) * (y2 - y1)\n",
        "\n",
        "    # sort the bounding boxes \n",
        "    idxs = np.argsort(probs)\n",
        "\n",
        "    # keep looping while some indexes still remain in the indexes\n",
        "    # list\n",
        "    while len(idxs) > 0:\n",
        "        # grab the last index in the indexes list and add the\n",
        "        # index value to the list of picked indexes\n",
        "        last = len(idxs) - 1\n",
        "        i = idxs[last]\n",
        "        pick.append(i)\n",
        "\n",
        "        # find the intersection\n",
        "\n",
        "        xx1_int = np.maximum(x1[i], x1[idxs[:last]])\n",
        "        yy1_int = np.maximum(y1[i], y1[idxs[:last]])\n",
        "        xx2_int = np.minimum(x2[i], x2[idxs[:last]])\n",
        "        yy2_int = np.minimum(y2[i], y2[idxs[:last]])\n",
        "\n",
        "        ww_int = np.maximum(0, xx2_int - xx1_int)\n",
        "        hh_int = np.maximum(0, yy2_int - yy1_int)\n",
        "\n",
        "        area_int = ww_int * hh_int\n",
        "\n",
        "        # find the union\n",
        "        area_union = area[i] + area[idxs[:last]] - area_int\n",
        "\n",
        "        # compute the ratio of overlap\n",
        "        overlap = area_int/(area_union + 1e-6)\n",
        "\n",
        "        # delete all indexes from the index list that have\n",
        "        idxs = np.delete(idxs, np.concatenate(([last],\n",
        "            np.where(overlap > overlap_thresh)[0])))\n",
        "\n",
        "        if len(pick) >= max_boxes:\n",
        "            break\n",
        "\n",
        "    # return only the bounding boxes that were picked using the integer data type\n",
        "    boxes = boxes[pick].astype(\"int\")\n",
        "    probs = probs[pick]\n",
        "    return boxes, probs\n",
        "\n",
        "\n",
        "        w1 = np.round(w1)\n",
        "        h1 = np.round(h1)\n",
        "        return np.stack([x1, y1, w1, h1])\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return X\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00023-4d7af10d-22a9-4f30-895b-09007cfb0f55",
        "deepnote_cell_type": "text-cell-h1",
        "id": "dVKDLeJkr9Yi"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00024-0fc0e004-b44a-45ff-b1f5-34bf387d527f",
        "deepnote_cell_type": "markdown",
        "id": "f8vYuccpr9Yj"
      },
      "source": [
        "This is our training part of the code. Here we demonstrate the training process by training a model for almond detection. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-9uuYijv-iF",
        "cell_id": "00018-a77cb637-3383-4f69-95aa-67e6dbbddc2d",
        "deepnote_cell_type": "code"
      },
      "source": [
        "#------Change path to fit one's own needs.\n",
        "base_path = './'\n",
        "\n",
        "train_path1 =  './almonds/' # data folder\n",
        "#------Change path to fit one's own needs.\n",
        "\n",
        "num_rois = 4 # Number of RoIs to process at once.\n",
        "\n",
        "# Augmentation flag\n",
        "horizontal_flips = True # Augment with horizontal flips in training. \n",
        "vertical_flips = True   # Augment with vertical flips in training. \n",
        "rot_90 = True           # Augment with 90 degree rotations in training. \n",
        "\n",
        "#------Change path of weight files, configuration files, and record files\n",
        "output_weight_path = os.path.join(base_path, 'model/model_frcnn_vgg.hdf5')\n",
        "\n",
        "record_path = os.path.join(base_path, 'model/record.csv') # Record data (used to save the losses, classification accuracy and mean average precision)\n",
        "\n",
        "base_weight_path = os.path.join(base_path, 'model/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n",
        "\n",
        "config_output_filename = os.path.join(base_path, 'model_vgg_config.pickle')\n",
        "#------Change path of weight files, configuration files, and record files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00026-095a87c1-8aff-4df2-a060-5c86286bab40",
        "deepnote_cell_type": "markdown",
        "id": "EuTMQK-0r9Yj"
      },
      "source": [
        "Here we apply the same data augmentation as suggested in <a href=\"https://arxiv.org/abs/1610.03677\">Deep Fruit Detection in Orchards</a> by setting up the correct configurations. To be more specific, we perform data augmentation with a 90° rotation on training images with both horizontal and vertical flips."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqnuh-mcwsAN",
        "cell_id": "00019-9bf9edaa-3dde-41ef-bb80-690dd79ce166",
        "deepnote_cell_type": "code"
      },
      "source": [
        "C = Config()\n",
        "\n",
        "C.use_horizontal_flips = horizontal_flips\n",
        "C.use_vertical_flips = vertical_flips\n",
        "C.rot_90 = rot_90\n",
        "\n",
        "C.record_path = record_path\n",
        "C.model_path = output_weight_path\n",
        "C.num_rois = num_rois\n",
        "\n",
        "C.base_net_weights = base_weight_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00028-9dc9c9d5-663d-48be-863a-5e4a2cdfaf44",
        "deepnote_cell_type": "markdown",
        "id": "2_Yzfrror9Yk"
      },
      "source": [
        "Keep track of the time loading images, as sometimes it can be really slow on Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g1ynnOrwv97",
        "cell_id": "00020-32f8ebf5-633b-4054-8188-d7fe423783f6",
        "output_cleared": true,
        "source_hash": null,
        "deepnote_cell_type": "code"
      },
      "source": [
        "st = time.time()\n",
        "train_imgs, classes_count, class_mapping = get_data(train_path1)\n",
        "print()\n",
        "print('Spend %0.2f mins to load the data' % ((time.time()-st)/60) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00030-d20055ac-550e-4615-b536-719b29003ff1",
        "deepnote_cell_type": "markdown",
        "id": "Z0gYJ-uyr9Yl"
      },
      "source": [
        "Examine whether our data was correctly loaded. Save the configuration for testing on the same model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tg247O0_1y",
        "cell_id": "00021-63979534-413d-461e-81b0-55102100edbf",
        "output_cleared": true,
        "source_hash": null,
        "deepnote_cell_type": "code"
      },
      "source": [
        "if 'bg' not in classes_count:\n",
        "\tclasses_count['bg'] = 0\n",
        "\tclass_mapping['bg'] = len(class_mapping)\n",
        "# e.g.\n",
        "#    classes_count: {'Car': 2383, 'Mobile phone': 1108, 'Person': 3745, 'bg': 0}\n",
        "#    class_mapping: {'Person': 0, 'Car': 1, 'Mobile phone': 2, 'bg': 3}\n",
        "C.class_mapping = class_mapping\n",
        "\n",
        "print('Training images per class:')\n",
        "pprint.pprint(classes_count)\n",
        "print('Num classes (including bg) = {}'.format(len(classes_count)))\n",
        "print(class_mapping)\n",
        "\n",
        "# Save the configuration\n",
        "with open(config_output_filename, 'wb') as config_f:\n",
        "\tpickle.dump(C,config_f)\n",
        "\tprint('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00032-3bf85f06-b594-4ca2-81c0-99eabfb051e9",
        "deepnote_cell_type": "markdown",
        "id": "CqB-WGo6r9Ym"
      },
      "source": [
        "Randomly shuffle the training images, always using random seed 42 for reproducible results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl01SlSa3lZj",
        "cell_id": "00022-5d8a3430-fadf-43b1-a109-42126d00ee48",
        "output_cleared": true,
        "source_hash": null,
        "deepnote_cell_type": "code"
      },
      "source": [
        "# Shuffle the images with seed\n",
        "random.seed(42)\n",
        "random.shuffle(train_imgs)\n",
        "\n",
        "print('Num train samples (images) {}'.format(len(train_imgs)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00034-9bb6229f-195e-4b01-b10f-c1b3f124831f",
        "deepnote_cell_type": "markdown",
        "id": "8Lweb5vDr9Ym"
      },
      "source": [
        "Get anchors for training images' ground truth on bounding boxes (locations, size). Use 'next' to generate iterables for getting new data to save memory for large training sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JbvP5hO3uRp",
        "cell_id": "00023-e638e43b-cc08-4d8e-89de-d6aa6b733f35",
        "deepnote_cell_type": "code"
      },
      "source": [
        "data_gen_train = get_anchor_gt(train_imgs, C, get_img_output_length, mode='train')\n",
        "X, Y, image_data, debug_img, debug_num_pos = next(data_gen_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00036-7e56d28b-794a-42c1-9b61-c800828a546b",
        "deepnote_cell_type": "markdown",
        "id": "Dja7Vmogr9Yn"
      },
      "source": [
        "Now we can take a look at our ground truth bounding boxes in the training image!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg_7CVARtiFs"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1b0WsRNZjfJcrOOOXNSE9eiwLRiBJqlia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00061-0b604415-65df-47ca-badc-34eb6dd06b25",
        "deepnote_cell_type": "markdown",
        "id": "8XRtR24er9Yo"
      },
      "source": [
        "Initialise our network by setting up correct parameters as suggested by <a href=\"https://arxiv.org/abs/1610.03677\">Deep Fruit Detection in Orchards</a>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxPdZSlD37-4",
        "cell_id": "00026-b3c2db20-e943-49e5-a138-d215c153c7d2",
        "deepnote_cell_type": "code"
      },
      "source": [
        "input_shape_img = (None, None, 3)\n",
        "\n",
        "img_input = Input(shape=input_shape_img)\n",
        "roi_input = Input(shape=(None, 4))\n",
        "\n",
        "# define the base network (VGG16 here, can be Resnet50, Inception, etc)\n",
        "shared_layers = nn_base(img_input, trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WewcIL3q4EGk",
        "cell_id": "00027-ec004db1-f4c0-4adb-9436-6c630f73618d",
        "output_cleared": true,
        "source_hash": null,
        "deepnote_cell_type": "code"
      },
      "source": [
        "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios) # 9\n",
        "rpn = rpn_layer(shared_layers, num_anchors)\n",
        "\n",
        "classifier = classifier_layer(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count))\n",
        "\n",
        "model_rpn = Model(img_input, rpn[:2])\n",
        "model_classifier = Model([img_input, roi_input], classifier)\n",
        "\n",
        "# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
        "model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
        "\n",
        "# Because the google colab can only run the session several hours one time (then you need to connect again), \n",
        "# we need to save the model and load the model to continue training\n",
        "if not os.path.isfile(C.model_path):\n",
        "    #If this is the begin of the training, load the pre-traind base network such as vgg-16\n",
        "    try:\n",
        "        print('This is the first time of your training')\n",
        "        print('loading weights from {}'.format(C.base_net_weights))\n",
        "        model_rpn.load_weights(C.base_net_weights, by_name=True)\n",
        "        model_classifier.load_weights(C.base_net_weights, by_name=True)\n",
        "    except:\n",
        "        print('Could not load pretrained model weights. Weights can be found in the keras application folder \\\n",
        "            https://github.com/fchollet/keras/tree/master/keras/applications')\n",
        "    \n",
        "    # Create the record.csv file to record losses, acc and mAP\n",
        "    record_df = pd.DataFrame(columns=['mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr', 'curr_loss', 'elapsed_time', 'mAP'])\n",
        "else:\n",
        "    # If this is a continued training, load the trained model from before\n",
        "    print('Continue training based on previous trained model')\n",
        "    print('Loading weights from {}'.format(C.model_path))\n",
        "    model_rpn.load_weights(C.model_path, by_name=True)\n",
        "    model_classifier.load_weights(C.model_path, by_name=True)\n",
        "    \n",
        "    # Load the records\n",
        "    record_df = pd.read_csv(record_path)\n",
        "\n",
        "    r_mean_overlapping_bboxes = record_df['mean_overlapping_bboxes']\n",
        "    r_class_acc = record_df['class_acc']\n",
        "    r_loss_rpn_cls = record_df['loss_rpn_cls']\n",
        "    r_loss_rpn_regr = record_df['loss_rpn_regr']\n",
        "    r_loss_class_cls = record_df['loss_class_cls']\n",
        "    r_loss_class_regr = record_df['loss_class_regr']\n",
        "    r_curr_loss = record_df['curr_loss']\n",
        "    r_elapsed_time = record_df['elapsed_time']\n",
        "    r_mAP = record_df['mAP']\n",
        "\n",
        "    print('Already train %dK batches'% (len(record_df)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sOyAN8v4IA0",
        "cell_id": "00028-f34fa7a2-58c5-4c9c-a65c-be5093035b1e",
        "deepnote_cell_type": "code"
      },
      "source": [
        "optimizer = Adam(lr=1e-5)\n",
        "optimizer_classifier = Adam(lr=1e-5)\n",
        "model_rpn.compile(optimizer=optimizer, loss=[rpn_loss_cls(num_anchors), rpn_loss_regr(num_anchors)])\n",
        "model_classifier.compile(optimizer=optimizer_classifier, loss=[class_loss_cls, class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
        "model_all.compile(optimizer='sgd', loss='mae')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00041-09b6cedb-89a2-494d-aa12-4af534c72a31",
        "deepnote_cell_type": "markdown",
        "id": "rTXFN4-tr9Yq"
      },
      "source": [
        "Due to the limitations of our group's computing power at hand, we only train this model with 70 epochs with the length of 100 for each epoch. The following figures show the training result. They are mean number of the overleaping bboxes, accuracy, and losses of regression and classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuRIHFiQtkTg"
      },
      "source": [
        "![](https://drive.google.com/uc?id=10aQfkjg5tmMcMCTd-CgKGJmpAS4LPvt1)  \n",
        "![](https://drive.google.com/uc?id=1GhJRSYALbf7aIXUbFJqr9Pk36DG4FW4F)  \n",
        "![](https://drive.google.com/uc?id=1lP9oxoosQa9qBNWPGabzavCkAheKgzJ8)  \n",
        "![](https://drive.google.com/uc?id=14wI6gu0lC2dYfyDmmAggZzEurFp6c6An)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00047-b62351cd-fced-4540-9d2f-1e74fa0544c4",
        "deepnote_cell_type": "markdown",
        "id": "B-6GOipkr9Yr"
      },
      "source": [
        "As can be seen from these figures, our model is not fully converged because of the limited number of epochs. However, as our primary reproduction goal is to obtain the complete network of our reference paper, we believe this result is sufficient to show that our reconstructed model implements the correct architecture of the network and can be used to train fruit detectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00046-91007d07-f24f-4299-9fb4-0d1dd2503da9",
        "deepnote_cell_type": "text-cell-h3",
        "id": "iVaDXWM8r9Ys"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00048-15f98db5-e7ab-44bd-9064-3583da2a7008",
        "deepnote_cell_type": "markdown",
        "id": "EdKDHwCrr9Ys"
      },
      "source": [
        "The steps of testing is simple: \n",
        "\n",
        "1. Load the model's configuration and testing sets.\n",
        "2. For each testing image, give predicted bounding boxes.\n",
        "3. Compare the predicted bounding boxes with the ground truth label in terms of their locations and sizes.\n",
        "4. Compute mean average precision (MAP), as is done by the reference paper.\n",
        "\n",
        "The following code blocks first show one image with predicted bounding boxes before the non-maximum suppression (NMS), then one after the NMS. Along with the input image shown before, we complete the reproduction of Fig 2. in our main reference paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00048-e3f3200d-9eec-44eb-a0ce-18c78147897a",
        "deepnote_cell_type": "code",
        "id": "Pyqk625or9Yt"
      },
      "source": [
        "bbox_threshold = 0.1\n",
        "i = 1\n",
        "test_img_names_path = os.path.join(test_path1, 'sets/test.txt')\n",
        "test_img_path = os.path.join(test_path1, 'images')\n",
        "\n",
        "with open(test_img_names_path,'r') as f:\n",
        "    print('Parsing testing files')\n",
        "    lines = [line.rstrip('\\n') for line in f]\n",
        "    for line in lines: # img_name\n",
        "\n",
        "        # Print process\n",
        "        sys.stdout.write('\\r'+'idx=' + str(i))\n",
        "        i += 1\n",
        "        st = time.time()\n",
        "        filepath = os.path.join(test_img_path, line + '.png')\n",
        "        img = cv2.imread(filepath)\n",
        "        X, ratio = format_img(img, C)\n",
        "    \n",
        "        X = np.transpose(X, (0, 2, 3, 1))\n",
        "\n",
        "        # get output layer Y1, Y2 from the RPN and the feature maps F\n",
        "        # Y1: y_rpn_cls\n",
        "        # Y2: y_rpn_regr\n",
        "        [Y1, Y2, F] = model_rpn.predict(X)\n",
        "\n",
        "        # Get bboxes by applying NMS \n",
        "        # R.shape = (300, 4)\n",
        "        R = rpn_to_roi(Y1, Y2, C, K.image_data_format(), overlap_thresh=0.2)\n",
        "\n",
        "        # convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
        "        R[:, 2] -= R[:, 0]\n",
        "        R[:, 3] -= R[:, 1]\n",
        "\n",
        "        # apply the spatial pyramid pooling to the proposed regions\n",
        "        bboxes = {}\n",
        "        probs = {}\n",
        "        for jk in range(R.shape[0]//C.num_rois + 1):\n",
        "          ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n",
        "          if ROIs.shape[1] == 0:\n",
        "              break\n",
        "\n",
        "          if jk == R.shape[0]//C.num_rois:\n",
        "              #pad R\n",
        "              curr_shape = ROIs.shape\n",
        "              target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n",
        "              ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
        "              ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
        "              ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
        "              ROIs = ROIs_padded\n",
        "\n",
        "          [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n",
        "\n",
        "          # Calculate bboxes coordinates on resized image\n",
        "          for ii in range(P_cls.shape[1]):\n",
        "              # Ignore 'bg' class\n",
        "              if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
        "                  continue\n",
        "\n",
        "              cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n",
        "\n",
        "              if cls_name not in bboxes:\n",
        "                  bboxes[cls_name] = []\n",
        "                  probs[cls_name] = []\n",
        "\n",
        "              (x, y, w, h) = ROIs[0, ii, :]\n",
        "\n",
        "              cls_num = np.argmax(P_cls[0, ii, :])\n",
        "              try:\n",
        "                  (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n",
        "                  tx /= C.classifier_regr_std[0]\n",
        "                  ty /= C.classifier_regr_std[1]\n",
        "                  tw /= C.classifier_regr_std[2]\n",
        "                  th /= C.classifier_regr_std[3]\n",
        "                  x, y, w, h = apply_regr(x, y, w, h, tx, ty, tw, th)\n",
        "              except:\n",
        "                  pass\n",
        "              bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n",
        "              probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
        "\n",
        "        all_dets = []\n",
        "\n",
        "        for key in bboxes:\n",
        "            bbox = np.array(bboxes[key])\n",
        "\n",
        "            for ijk in range(bbox.shape[0]):\n",
        "                (x1, y1, x2, y2) = bbox[ijk,:]\n",
        "                #det = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': key, 'prob': new_probs[jk]}\n",
        "                #all_dets.append(det)\n",
        "\n",
        "                # Calculate real coordinates on original image\n",
        "                (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n",
        "                \n",
        "                cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),4)\n",
        "                \n",
        "        print('Elapsed time = {}'.format(time.time() - st))\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.grid()\n",
        "        plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
        "        plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00074-c5a857c9-076e-4127-a3de-4236aa97fb64",
        "deepnote_cell_type": "markdown",
        "id": "ANITiG-Dr9Yt"
      },
      "source": [
        "This is the pre-NMS prediction. Many bounding boxes are overlapping with others, which is exactly why we need to perform NMS to reduce the overlapping boxes and save one box for one fruit.  \n",
        "![](https://drive.google.com/uc?id=1pgfEyEWRv-472AgcncqxOu-qDGp9cDbc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00074-d4ae0bb9-44bf-499c-aeef-fc2ac54596bf",
        "deepnote_cell_type": "code",
        "id": "KEP59IAnr9Yu"
      },
      "source": [
        "        for key in bboxes:\n",
        "            bbox = np.array(bboxes[key])\n",
        "\n",
        "            new_boxes, new_probs = non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.3)\n",
        "            #print(new_boxes.shape[0])\n",
        "            for jk in range(new_boxes.shape[0]):\n",
        "                (x1, y1, x2, y2) = new_boxes[jk,:]\n",
        "                det = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': key, 'prob': new_probs[jk]}\n",
        "                all_dets.append(det)\n",
        "\n",
        "                # Calculate real coordinates on original image\n",
        "                (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n",
        "                \n",
        "                cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),4)\n",
        "\n",
        "                textLabel = '{}: {}'.format(key,int(100*new_probs[jk]))\n",
        "                all_dets.append((key,100*new_probs[jk]))\n",
        "\n",
        "                (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n",
        "                textOrg = (real_x1, real_y1-0)\n",
        "                \n",
        "                #cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 1)\n",
        "                #cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n",
        "                #cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00049-41447db7-574c-42fa-9eae-566ce0441a75",
        "deepnote_cell_type": "markdown",
        "id": "ZVheyG_br9Yu"
      },
      "source": [
        "Much better now, isn't it? This would be the end of our reproduction of fig 2. in the main reference paper as we finished building the complete network architecture and showing all intermediate and final results.  \n",
        "![](https://drive.google.com/uc?id=1OceP_Yf7n3V7Ol53AbMcEBFvbE4BYdVm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00051-65741a46-6432-4a10-b42a-5d9a2fc2a7fa",
        "deepnote_cell_type": "markdown",
        "id": "RN81sCZir9Yv"
      },
      "source": [
        "Determine whether the prediected bounding boxes match with the ground truth label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00051-235e6857-15c9-42a4-b3da-756c27f436e8",
        "deepnote_cell_type": "code",
        "id": "wSaK5_hYr9Yv"
      },
      "source": [
        "def get_map(pred, gt, f):\n",
        "\tT = {}\n",
        "\tP = {}\n",
        "\tfx, fy = f\n",
        "\n",
        "\tfor bbox in gt:\n",
        "\t\tbbox['bbox_matched'] = False\n",
        "\n",
        "\tpred_probs = np.array([s['prob'] for s in pred])\n",
        "\tbox_idx_sorted_by_prob = np.argsort(pred_probs)[::-1]\n",
        "\n",
        "\tfor box_idx in box_idx_sorted_by_prob:\n",
        "\t\tpred_box = pred[box_idx]\n",
        "\t\tpred_class = pred_box['class']\n",
        "\t\tpred_x1 = pred_box['x1']\n",
        "\t\tpred_x2 = pred_box['x2']\n",
        "\t\tpred_y1 = pred_box['y1']\n",
        "\t\tpred_y2 = pred_box['y2']\n",
        "\t\tpred_prob = pred_box['prob']\n",
        "\t\tif pred_class not in P:\n",
        "\t\t\tP[pred_class] = []\n",
        "\t\t\tT[pred_class] = []\n",
        "\t\tP[pred_class].append(pred_prob)\n",
        "\t\tfound_match = False\n",
        "\n",
        "\t\tfor gt_box in gt:\n",
        "\t\t\tgt_class = gt_box['class']\n",
        "\t\t\tgt_x1 = gt_box['x1']/fx\n",
        "\t\t\tgt_x2 = gt_box['x2']/fx\n",
        "\t\t\tgt_y1 = gt_box['y1']/fy\n",
        "\t\t\tgt_y2 = gt_box['y2']/fy\n",
        "\t\t\tgt_seen = gt_box['bbox_matched']\n",
        "\t\t\tif gt_class != pred_class:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tif gt_seen:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tiou_map = iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))\n",
        "\t\t\tif iou_map >= 0.2: # according to deep fruit paper, this is enough\n",
        "\t\t\t\tfound_match = True\n",
        "\t\t\t\tgt_box['bbox_matched'] = True\n",
        "\t\t\t\tbreak\n",
        "\t\t\telse:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\tT[pred_class].append(int(found_match))\n",
        "\n",
        "\tfor gt_box in gt:\n",
        "\t\tif not gt_box['bbox_matched']:# and not gt_box['difficult']:\n",
        "\t\t\tif gt_box['class'] not in P:\n",
        "\t\t\t\tP[gt_box['class']] = []\n",
        "\t\t\t\tT[gt_box['class']] = []\n",
        "\n",
        "\t\t\tT[gt_box['class']].append(1)\n",
        "\t\t\tP[gt_box['class']].append(0)\n",
        "\n",
        "\treturn T, P"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00053-05f85936-683b-40fc-ba7c-92ea2270bb8f",
        "deepnote_cell_type": "markdown",
        "id": "5bOU4Pjzr9Yv"
      },
      "source": [
        "Now we can compute the MAP score according to the calculated bounding boxes for images and their corresponding ground truth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00053-9a7a61e4-ee32-4e2f-85d6-00c55ecbe70d",
        "deepnote_cell_type": "code",
        "id": "sS4HsUiEr9Yw"
      },
      "source": [
        "T = {}\n",
        "P = {}\n",
        "mAPs = []\n",
        "for idx, img_data in enumerate(test_imgs):\n",
        "    print('{}/{}'.format(idx,len(test_imgs)))\n",
        "    st = time.time()\n",
        "    filepath = img_data['filepath']\n",
        "\n",
        "    img = cv2.imread(filepath)\n",
        "\n",
        "    X, fx, fy = format_img_map(img, C)\n",
        "\n",
        "    # Change X (img) shape from (1, channel, height, width) to (1, height, width, channel)\n",
        "    X = np.transpose(X, (0, 2, 3, 1))\n",
        "\n",
        "    # get the feature maps and output from the RPN\n",
        "    [Y1, Y2, F] = model_rpn.predict(X)\n",
        "\n",
        "\n",
        "    R = rpn_to_roi(Y1, Y2, C, K.image_data_format(), overlap_thresh=0.7)\n",
        "\n",
        "    # convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
        "    R[:, 2] -= R[:, 0]\n",
        "    R[:, 3] -= R[:, 1]\n",
        "\n",
        "    # apply the spatial pyramid pooling to the proposed regions\n",
        "    bboxes = {}\n",
        "    probs = {}\n",
        "\n",
        "    for jk in range(R.shape[0] // C.num_rois + 1):\n",
        "        ROIs = np.expand_dims(R[C.num_rois * jk:C.num_rois * (jk + 1), :], axis=0)\n",
        "        if ROIs.shape[1] == 0:\n",
        "            break\n",
        "\n",
        "        if jk == R.shape[0] // C.num_rois:\n",
        "            # pad R\n",
        "            curr_shape = ROIs.shape\n",
        "            target_shape = (curr_shape[0], C.num_rois, curr_shape[2])\n",
        "            ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
        "            ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
        "            ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
        "            ROIs = ROIs_padded\n",
        "\n",
        "        [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n",
        "\n",
        "        # Calculate all classes' bboxes coordinates on resized image (300, 400)\n",
        "        # Drop 'bg' classes bboxes\n",
        "        for ii in range(P_cls.shape[1]):\n",
        "\n",
        "            # If class name is 'bg', continue\n",
        "            if np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
        "                continue\n",
        "\n",
        "            # Get class name\n",
        "            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n",
        "\n",
        "            if cls_name not in bboxes:\n",
        "                bboxes[cls_name] = []\n",
        "                probs[cls_name] = []\n",
        "\n",
        "            (x, y, w, h) = ROIs[0, ii, :]\n",
        "\n",
        "            cls_num = np.argmax(P_cls[0, ii, :])\n",
        "            try:\n",
        "                (tx, ty, tw, th) = P_regr[0, ii, 4 * cls_num:4 * (cls_num + 1)]\n",
        "                tx /= C.classifier_regr_std[0]\n",
        "                ty /= C.classifier_regr_std[1]\n",
        "                tw /= C.classifier_regr_std[2]\n",
        "                th /= C.classifier_regr_std[3]\n",
        "                x, y, w, h = roi_helpers.apply_regr(x, y, w, h, tx, ty, tw, th)\n",
        "            except:\n",
        "                pass\n",
        "            bboxes[cls_name].append([16 * x, 16 * y, 16 * (x + w), 16 * (y + h)])\n",
        "            probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
        "\n",
        "    all_dets = []\n",
        "\n",
        "    for key in bboxes:\n",
        "        bbox = np.array(bboxes[key])\n",
        "\n",
        "        # Apply non-max-suppression on final bboxes to get the output bounding boxe\n",
        "        new_boxes, new_probs = non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.3)\n",
        "        for jk in range(new_boxes.shape[0]):\n",
        "            (x1, y1, x2, y2) = new_boxes[jk, :]\n",
        "            det = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': key, 'prob': new_probs[jk]}\n",
        "            all_dets.append(det)\n",
        "\n",
        "\n",
        "    print('Elapsed time = {}'.format(time.time() - st))\n",
        "    t, p = get_map(all_dets, img_data['bboxes'], (fx, fy))\n",
        "    for key in t.keys():\n",
        "        if key not in T:\n",
        "            T[key] = []\n",
        "            P[key] = []\n",
        "        T[key].extend(t[key])\n",
        "        P[key].extend(p[key])\n",
        "    all_aps = []\n",
        "    for key in T.keys():\n",
        "        ap = average_precision_score(T[key], P[key])\n",
        "        print('{} AP: {}'.format(key, ap))\n",
        "        all_aps.append(ap)\n",
        "    print('mAP = {}'.format(np.mean(np.array(all_aps))))\n",
        "    mAPs.append(np.mean(np.array(all_aps)))\n",
        "\n",
        "print()\n",
        "print('mean average precision:', np.mean(np.array(mAPs)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00055-b71304e7-2354-4b30-aafe-1d0fa047e658",
        "deepnote_cell_type": "markdown",
        "id": "wXvAtZNZr9Yw"
      },
      "source": [
        "For this test, our MAP score is 0.66, which is slightly less than 0.72 from our reference paper. Considering we only train our model with 70 epochs and our model is not fully converged yet, we believe our results basically match with the paper's results, proving our implementation of the network architecture is correct.\n",
        "\n",
        "Above is our reproduction of Fig. 2 of <a href=\"https://arxiv.org/abs/1610.03677\">Deep Fruit Detection in Orchards</a>, which fits the criterion of \"new code variant\". \n",
        "\n",
        "We borrowed a page from this <a href=\"Faster RCNN</a>https://github.com/rbgirshick/py-faster-rcnn\">Faster RCNN</a>. However, our implementation is different in:\n",
        "\n",
        "1. The faster RCNN code is not straightforward runable on our computers because it contains lots of out-dated functions and also needs strict configurations in terms software versions and hardware requirements. This makes the original code very difficult for beginners to grasp the essence of it as setting up the correct environment to merely run its demo is already taxing work. We adapt this code so we can directly run it on Google Colab, making our code readable even for beginners.\n",
        "\n",
        "2. We reimplement the network architecture with TensorFlow. We make necessary chages so the model's parameters are the same as which suggested by  <a href=\"https://arxiv.org/abs/1610.03677\">Deep Fruit Detection in Orchards</a>. We rewrite the data loader, training and testing parts to fit our own training/testing purposes for fruit detection.  \n",
        "\n",
        "We then perform ablation study from two aspects: transfer learning and data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00049-62bca1fa-d8ef-4e67-81a1-35a608c669dc",
        "deepnote_cell_type": "text-cell-h3",
        "id": "0G_6y0O6r9Yw"
      },
      "source": [
        "### Ablation Study 1: Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00050-38b41b7b-d729-43ff-89bf-8b6a434fceeb",
        "deepnote_cell_type": "markdown",
        "id": "3c1olcmdr9Yx"
      },
      "source": [
        "For transfer learning, we compare the pre-trained model from almond with a model directly initialised from ImageNet. The utility of transfer learning can be examined by the detection performance of both models on the set of apples. Instead of comparing the detection performance v.s the number of training images as the reference paper does, we want to find out whether transfer learning is still effective when the pre-trained model is not fully converged due to a lack of computing resources. \n",
        "\n",
        "Our result shows that the MAP score for the transfer learning model is 0.79 and 0.71 for model initialised from ImageNet. The MAP Score is computed from the average of 3 trials each has 70 epochs with a length of 100 steps. Hence, we believe transfer learning may still be effective even when the pre-trained model is not fully converged.\n",
        "\n",
        "|  Trial  | MAP(Pre-trained) | MAP(ImageNet) |\n",
        "|:-------:|:----------------:|:-------------:|\n",
        "|    1    |       0.81       |      0.72     |\n",
        "|    2    |       0.78       |      0.71     |\n",
        "|    3    |       0.79       |      0.72     |\n",
        "| Average |       0.79       |      0.72     |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00070-03b40d71-7804-4b93-8fb7-36b00f41ab79",
        "deepnote_cell_type": "text-cell-h3",
        "id": "OxIxKxdar9Yx"
      },
      "source": [
        "### Ablation Study 2: Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00071-b806b8ac-5052-4766-a5b5-81d20972f05f",
        "deepnote_cell_type": "markdown",
        "id": "xsqMGo_Ir9Yx"
      },
      "source": [
        "Data augmentation is a common way to expand the variability of the training data by artificially enlarging the dataset using label-preserving transformations. The process increases the networks capability to generalise and reduces overfitting. Here we apply horizontal flips, vertical flips and rotation of 90 degree on the training set. In the origin paper, the authors applied data augmentation on apple and mango detection. We want to explore the effect of data augmentation when it comes to almond detection.\n",
        "\n",
        "Our result shows that the MAP score for the model with and without data augmentation is 0.70 and 0.71, respectively. For apple and mango detection, the authors found that in most cases, with data augmentation, the network reached a fixed detection performance with less than half the number of training samples. Data augmentation does not improve the score significantly for almond detection. The reason of this may be the fact that almonds have small size, making it hard for the network to classify them. Hence, the data augmentation cannot improve the performance for almond detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00082-2824db2c-9198-4297-a23c-3b38a122dceb",
        "deepnote_cell_type": "text-cell-h3",
        "id": "wlbgYOlQr9Yx"
      },
      "source": [
        "### Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00083-25a5cbc5-d620-40ea-99ae-33f7e7c4025e",
        "deepnote_cell_type": "markdown",
        "id": "w7woQfkOr9Yy"
      },
      "source": [
        "During the project, we explore the architecture of Faster R-CNN and create a Faster R-CNN model for fruit detection. As reported in previous sections，our model's MAP score detecting almonds is 0.66, which is slightly less than 0.72 in our reference paper due to limited number of training epochs. This score confirms the conslusion in the origin paper that Faster R-CNN works well on fruit detection, also proves our implementation is correct as our intermiediate and final results resembles what are shown in Fig 2 in the reference paper. \n",
        "\n",
        "After reproducing Fig 2, We perform the ablation study on transfer learning and data augmentation. The former one indicates that our model pre-trained using almond dataset works better for detecting apples compared to a model with initial weights from ImageNet when the number of epochs of transfer learning limited. This makes sense as the pre-trained model provides a better prior knowledge for fruit detection, which also suggests the detection of apples and almonds may be similar in some way. The ablation study on the effects of data augmentation shows that data augmentation for almond detection cannot significantly improve the detection performance of apples."
      ]
    }
  ]
}